[General]
; Where to poll the commands from:
PublicWebserver=https://your.server.com/
; Admin user (after you created one based on the installation instructions):
AdminUsername=admin
; Password of said admin user (make sure it's not used anywhere else):
AdminPassword=admin

; Send server messages at least this many milliseconds apart to prevent
; accidental DoS or Cloudflare throttling:
ServerPollInterval=1500

; When multiple configuration files are present in the program directory, higher
; weights take priority. Negative numbers disable the corresponding engine, so it
; could be, for example, ran on a different computer to share the load.
; TL;DR: set these to 1 in the config file you want to use:
ChatWeight=-1
ImageGenWeight=-1
; CPU and GPU use the same memory (true or false):
Unified=true

[Chat]
; llama.cpp CPU release build download path (if 404, update build number):
LlamaCppCPUDownload=https://github.com/ggml-org/llama.cpp/releases/download/b4735/llama-b4735-bin-macos-arm64.zip
; llama.cpp GPU release build download path (if 404, update build number):
LlamaCppGPUDownload=https://github.com/ggml-org/llama.cpp/releases/download/b4735/llama-b4735-bin-macos-arm64.zip
; llama.cpp CUDA runtime download path (if 404, update build number):
LlamaCppCUDADownload=https://github.com/ggml-org/llama.cpp/releases/download/b4735/cudart-llama-bin-win-cu12.4-x64.zip

; For each new model you want to use, just add a new Model with a number one
; more in the end than the last. At least one model is mandatory in this file.
Model1=Chat
; In SLM mode, this is the small version of each model running on the CPU:
Model1SLM=https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-F16.gguf
; In LLM mode, this is the larger version of each model running on the GPU:
Model1LLM=https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-F16.gguf
; Describe how the model should behave:
Model1SystemMessage=You are a helpful assistant.

Model2=Think
Model2SLM=https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf
Model2LLM=https://huggingface.co/unsloth/QwQ-32B-GGUF/resolve/main/QwQ-32B-Q5_K_M.gguf
Model2SystemMessage=You are a helpful assistant.

Model3=Code
Model3SLM=https://huggingface.co/unsloth/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q8_0.gguf
Model3LLM=https://huggingface.co/unsloth/Qwen2.5-Coder-32B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-32B-Instruct-Q5_K_M.gguf
Model3SystemMessage=You are a helpful coding assistant always using the latest technologies.

[Image generation]
; Stable Diffusion WebUI release build download path:
WebUIDownload=https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/download/v1.10.0/sd.webui-1.10.1-blackwell.7z

; Base image size for non-HD generations:
ImageSize=1024x1024
; Image size for horizontal/landscape non-HD generations:
ImageSizeH=1344x768
; Image size for vertical/portrait non-HD generations:
ImageSizeV=768x1344
; Base image size for HD generations:
ImageSizeHD=1536x1536
; Image size for horizontal/landscape HD generations:
ImageSizeHDH=2048x1152
; Image size for vertical/portrait HD generations:
ImageSizeHDV=1152x2048

; About keywords and selectors: if they are present in a prompt, they trigger a
; change, like making the image widescreen. Keywords are single words (can't have
; spaces), but if they are found, the prompts won't change. Selectors are only
; detected if they're separated by a comma from the rest of the prompt, and will
; be removed from the final prompt that is processed.
; Keywords and selectors for making the image horizontal/landscape:
HKeywords=cinematic, landscape, scenic
HSelectors=horizontal, widescreen
; Keywords and selectors for making the image vertical/portrait:
VKeywords=portrait
VSelectors=vertical
; Keywords and selectors for making the image HD (keywords are disencouraged):
HDKeywords=
HDSelectors=4k, 8k, full HD, HD, high detail, high res, high-res, high resolution, high-resolution

; Below this line, Artist will refer to Stable Diffusion checkpoints. VoidMoA
; will swap these depending on the prompt.
; Default artist to use:
DefaultArtist=https://civitai.com/api/download/models/354657?type=Model&format=SafeTensor&size=full&fp=fp16
; Things not to have in the images (embedding files also work):
DefaultNegative=https://civitai.com/api/download/models/77169?type=Model&format=PickleTensor, nsfw, erotic

; Additional artists begin here. For each artist, there are 3 variables:
; - ArtistX - download path to the Stable Diffusion checkpoint file
; - ArtistXKeywords - if a keyword is present in the prompt, artist is selected
; - ArtistXSelector - if one is comma-separated in the prompt, artist is selected
; - ArtistXNegative - negative prompt, links for negative embedding downloads
; Selectors are removed from the prompt and can have spaces. keywords remain, but
; can't have spaces. Either list can be empty.
; If you want to add another artist with its keywords and selectors, just add new
; variables of the same names with a one larger number.
Artist1=https://civitai.com/api/download/models/479715?type=Model&format=SafeTensor&size=pruned&fp=fp16
Artist1Keywords=lifelike, photo, photographic, photography, photoreal, photorealistic, real, realistic
Artist1Selectors=photo, photoreal, photorealistic
Artist1Negative=https://civitai.com/api/download/models/77169?type=Model&format=PickleTensor, https://civitai.com/api/download/models/77173?type=Model&format=PickleTensor, nsfw, erotic

[Folders]
; All folders can be any path, including absolute paths.
; Where llama.cpp's CPU version is unpacked to:
LlamaCppCPURoot=llama.cpp AVX2
; Where llama.cpp's GPU version is unpacked to:
LlamaCppGPURoot=llama.cpp CUDA
; The folder where Large Language Models (LLMs) are saved:
Models=Chat models
; Where AUTOMATIC1111's Stable Diffusion WebUI is unpacked to:
WebUIRoot=Stable Diffusion WebUI
; Put artists to this folder (can be any path, even absolute):
Artists=Artists
; Put negative embeddings to this folder (can be any path, even absolute):
Embeddings=Embeddings

[Advanced settings for chat]
; Use this port for chat:
LlamaCppPort=64100
; If chat replies are not done in this many seconds, cancel the generation:
ChatTimeout=90

; Folder of extra knowledge for the chat (readme tells you how to add these):
ContextDocTree=Context Doc Tree
; Load all loosely matching context docs (requires large context windows):
FullContext=true
; Only search context docs for the latest prompt (huge performance gain):
AugmentLatestOnly=false
; Also add the system prompt to each augmentation evaluation. One good use case is
; adding a trigger word for a folder and limiting augmentation that way to a
; single model.
AugmentWithSystemPrompt=false

[Advanced settings for image generation]
; Use this port for image generation:
WebUIPort=64101
; If images are not done in this many seconds, cancel the generation:
ImageGenTimeout=60
; Number of times the image is refined for better results:
ImageGenSteps=4
; How close the prompts are followed, read your models' recommendation:
ImageGenGuidance=1.5

[Others]
; Comma-separated list of enabled extensions:
Extensions=
; The lowest severity logged message (can be Debug, Info, Warning, or Error):
LogLevel=Info